{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oUuQjW2oNMcJ",
        "qFfOrktMPq8M",
        "KE7kSYSXQYLf",
        "lGI4TNXPamMr",
        "RlgP1ytnRtUK",
        "KLmxLQeJSb4A",
        "kXbSKFyeMqr2",
        "k2-Fdp73cF0V",
        "V68C4cDySyek",
        "CKRRbwDFv3ZQ",
        "G3KBe4R65bl1",
        "yVJA-3jSATGV",
        "BbosNo0TD3oH",
        "Sam22f-YT1xR",
        "IF6-Z5RotAcO"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNfkkxUWUfQZDRXeUzVwLq/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/groda/Chart.js/blob/master/Hadoop_Setting_up_a_Single_Node_Cluster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HDFS and MapReduce on a single-node Hadoop cluster\n",
        "\n",
        "We're going to set up a single-node cluster (following the instructions on https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html) and show how to run simple HDFS and MapReduce commands.\n",
        "\n",
        "After downloading the software, it is necessary to carry out some preliminary steps like setting environment variables, generating SSH keys, etc.). We grouped all these steps under \"Prologue\".\n",
        "\n",
        "Once done with the prologue, we are able to start a single-node Hadoop cluster on the current virtual machine.\n",
        "\n",
        "We are going to run some test HDFS commands and MapReduce jobs on the Hadoop cluster.\n",
        "\n",
        "Finally, the cluster will be shut down.\n"
      ],
      "metadata": {
        "id": "oEF3qldGPj3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prologue"
      ],
      "metadata": {
        "id": "oUuQjW2oNMcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check the available Java version\n",
        " Apache Hadoop 3.3 and upper supports Java 8 and Java 11 (runtime only). See: https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions\n"
      ],
      "metadata": {
        "id": "qFfOrktMPq8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if Java version is one of `8`, `11`"
      ],
      "metadata": {
        "id": "EuWqBiV89ryq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7X0EZaMPrsD",
        "outputId": "22fd4e68-2df6-4fc9-e576-4aa2f2bb7222"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.16\" 2022-07-19\n",
            "OpenJDK Runtime Environment (build 11.0.16+8-post-Ubuntu-0ubuntu118.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.16+8-post-Ubuntu-0ubuntu118.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "JAVA_MAJOR_VERSION=$(java -version 2>&1 | grep -m1 -Po '(\\d+\\.)+\\d+' | cut -d '.' -f1)\n",
        "if [[ $JAVA_MAJOR_VERSION -eq 8 || $JAVA_MAJOR_VERSION -eq 11 ]]\n",
        " then \n",
        " echo \"Java version is one of 8, 11 ✓\" \n",
        " fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lABuOV124G4x",
        "outputId": "c5c262de-ef89-424a-c6fa-809aae857087"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java version is one of 8, 11 ✓\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the variable for the environment variable `JAVA_HOME`"
      ],
      "metadata": {
        "id": "pWROofISgKKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the path for the environment variable `JAVA_HOME`"
      ],
      "metadata": {
        "id": "uH4AGbkLP3iK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!readlink -f $(which java)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCmk5GOqv0Y-",
        "outputId": "f78355a5-7a0a-466b-cdb2-9ca6577dc798"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-11-openjdk-amd64/bin/java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract JAVA_HOME from the Java path by removing the `bin/java` part in the end"
      ],
      "metadata": {
        "id": "rGHKH3Vu9Nwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "JAVA_HOME=$(readlink -f $(which java) | sed 's/\\/bin\\/java$//')\n",
        "echo $JAVA_HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dd7en2Cv68ce",
        "outputId": "72286b87-01ca-4c76-9832-c45b7d83f90f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-11-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to use `JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64`.\n",
        "\n",
        "Use `%env%` to set the variable for the current notebook session."
      ],
      "metadata": {
        "id": "Hck9zJ3kQK2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%env JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64"
      ],
      "metadata": {
        "id": "P88xcreEQBcx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download core Hadoop \n",
        "Download the latest stable version of the core Hadoop distribution from one of the download mirrors locations https://www.apache.org/dyn/closer.cgi/hadoop/common/"
      ],
      "metadata": {
        "id": "KE7kSYSXQYLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "curl -L https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.3.4.tar.gz -o hadoop-3.3.4.tar.gz\n",
        "tar xzf hadoop-3.3.4.tar.gz "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C17WYI0mQRE8",
        "outputId": "dfc74a16-4b3d-4136-f3f4-916b015787b1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  9  663M    9 65.4M    0     0   189M      0  0:00:03 --:--:--  0:00:03  188M\r 51  663M   51  342M    0     0   256M      0  0:00:02  0:00:01  0:00:01  256M\r 94  663M   94  626M    0     0   268M      0  0:00:02  0:00:02 --:--:--  268M\r100  663M  100  663M    0     0   266M      0  0:00:02  0:00:02 --:--:--  266M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verify the downloaded file \n",
        "\n",
        "(see https://www.apache.org/dyn/closer.cgi/hadoop/common/)"
      ],
      "metadata": {
        "id": "lGI4TNXPamMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.3.4.tar.gz.sha512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr1QgjvXB-7y",
        "outputId": "0e8debed-2002-4ad2-8955-8ccc20430577"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   160  100   160    0     0   6666      0 --:--:-- --:--:-- --:--:--  6666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "sha512sum hadoop-3.3.4.tar.gz | cut - -d' ' -f1\n",
        "cut hadoop-3.3.4.tar.gz.sha512 -d' ' -f4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhTinHLqCrFQ",
        "outputId": "253dd2ae-a89e-4935-9e1b-1dbc4cd00e42"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ca5e12625679ca95b8fd7bb7babc2a8dcb2605979b901df9ad137178718821097b67555115fafc6dbf6bb32b61864ccb6786dbc555e589694a22bf69147780b4\n",
            "ca5e12625679ca95b8fd7bb7babc2a8dcb2605979b901df9ad137178718821097b67555115fafc6dbf6bb32b61864ccb6786dbc555e589694a22bf69147780b4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure `PATH`\n",
        "\n",
        "Add the Hadoop folder to the `PATH` environment variable\n"
      ],
      "metadata": {
        "id": "RlgP1ytnRtUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49xx-zSxIdxa",
        "outputId": "354acb16-14df-4158-9a5e-5ef44dcf59d4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#os.environ['HADOOP_HOME'] = os.path.join(os.environ['HOME'], 'hadoop-3.3.4')\n",
        "os.environ['HADOOP_HOME'] = os.path.join('/content', 'hadoop-3.3.4')\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'"
      ],
      "metadata": {
        "id": "6V03we10Igek"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.environ)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aif21X1ONvwH",
        "outputId": "c4c21478-f464-4cf2-ec51-2963415d33e4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "environ({'NV_LIBCUBLAS_DEV_VERSION': '11.4.1.1043-1', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-11-2', 'NV_CUDNN_PACKAGE_DEV': 'libcudnn8-dev=8.1.1.33-1+cuda11.2', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'NV_LIBNCCL_DEV_PACKAGE': 'libnccl-dev=2.8.4-1+cuda11.2', 'TCLLIBPATH': '/usr/share/tcltk/tcllib1.19', 'CLOUDSDK_PYTHON': 'python3', 'LANG': 'en_US.UTF-8', 'NV_LIBNPP_DEV_PACKAGE': 'libnpp-dev-11-2=11.3.2.152-1', 'HOSTNAME': '93ad6cb45e12', 'OLDPWD': '/', 'CLOUDSDK_CONFIG': '/content/.config', 'NV_LIBNPP_VERSION': '11.3.2.152-1', 'NV_NVPROF_DEV_PACKAGE': 'cuda-nvprof-11-2=11.2.152-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'NV_NVPROF_VERSION': '11.2.152-1', 'NV_LIBCUSPARSE_VERSION': '11.4.1.1152-1', 'DATALAB_SETTINGS_OVERRIDES': '{\"kernelManagerProxyPort\":6000,\"kernelManagerProxyHost\":\"172.28.0.3\",\"jupyterArgs\":[\"--ip=172.28.0.2\"],\"debugAdapterMultiplexerPath\":\"/usr/local/bin/dap_multiplexer\",\"enableLsp\":true}', 'NV_LIBCUBLAS_DEV_PACKAGE': 'libcublas-dev-11-2=11.4.1.1043-1', 'ENV': '/root/.bashrc', 'NCCL_VERSION': '2.8.4-1', 'TF_FORCE_GPU_ALLOW_GROWTH': 'true', 'NO_GCE_CHECK': 'False', 'PWD': '/', 'NVARCH': 'x86_64', 'HOME': '/root', 'NV_LIBCUSPARSE_DEV_VERSION': '11.4.1.1152-1', 'LAST_FORCED_REBUILD': '20220913', 'NV_LIBNCCL_PACKAGE_VERSION': '2.8.4-1', 'NV_LIBNCCL_PACKAGE': 'libnccl2=2.8.4-1+cuda11.2', 'DEBIAN_FRONTEND': 'noninteractive', 'NV_LIBNCCL_DEV_PACKAGE_NAME': 'libnccl-dev', 'NV_CUDA_LIB_VERSION': '11.2.2-1', 'NV_LIBNPP_PACKAGE': 'libnpp-11-2=11.3.2.152-1', 'NV_LIBNCCL_PACKAGE_NAME': 'libnccl2', 'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs', 'NV_NVTX_VERSION': '11.2.152-1', 'NV_LIBCUBLAS_VERSION': '11.4.1.1043-1', 'NV_LIBCUBLAS_PACKAGE': 'libcublas-11-2=11.4.1.1043-1', 'GCE_METADATA_TIMEOUT': '3', 'NV_CUDNN_VERSION': '8.1.1.33', 'NV_CUDA_CUDART_DEV_VERSION': '11.2.152-1', 'GLIBCPP_FORCE_NEW': '1', 'TBE_CREDS_ADDR': '172.28.0.1:8008', 'SHELL': '/bin/bash', 'GCS_READ_CACHE_BLOCK_SIZE_MB': '16', 'NV_NVML_DEV_VERSION': '11.2.152-1', 'PYTHONWARNINGS': 'ignore:::pip._internal.cli.base_command', 'CUDA_VERSION': '11.2.2', 'NV_LIBCUBLAS_PACKAGE_NAME': 'libcublas-11-2', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'TBE_RUNTIME_ADDR': '172.28.0.1:8011', 'SHLVL': '0', 'PYTHONPATH': '/env/python', 'NV_LIBCUBLAS_DEV_PACKAGE_NAME': 'libcublas-dev-11-2', 'NVIDIA_REQUIRE_CUDA': 'cuda>=11.2 brand=tesla,driver>=418,driver<419 brand=tesla,driver>=450,driver<451', 'NV_LIBNPP_DEV_VERSION': '11.3.2.152-1', 'TBE_EPHEM_CREDS_ADDR': '172.28.0.1:8009', 'NV_CUDA_CUDART_VERSION': '11.2.152-1', 'COLAB_GPU': '0', 'NV_CUDNN_PACKAGE_NAME': 'libcudnn8', 'GLIBCXX_FORCE_NEW': '1', 'PATH': '/content/hadoop-3.3.4/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin', 'NV_LIBNCCL_DEV_PACKAGE_VERSION': '2.8.4-1', 'LD_PRELOAD': '/usr/lib/x86_64-linux-gnu/libtcmalloc.so.4', 'NV_CUDNN_PACKAGE': 'libcudnn8=8.1.1.33-1+cuda11.2', 'JPY_PARENT_PID': '41', 'TERM': 'xterm-color', 'CLICOLOR': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline', 'ENABLE_DIRECTORYPREFETCHER': '1', 'USE_AUTH_EPHEM': '1', 'PYDEVD_USE_FRAME_EVAL': 'NO', 'HADOOP_HOME': '/content/hadoop-3.3.4', 'JAVA_HOME': '/usr/lib/jvm/java-11-openjdk-amd64'})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcM-idgZQqfV",
        "outputId": "db8d59aa-b5e7-4f6b-9c1b-272821c61d5c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hadoop-3.3.4/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure `core-site.xml` and `hdfs-site.xml`\n",
        "\n",
        "Edit the file `etc/hadoop/core-site.xml` and `etc/hadoop/hdfs-site.xml` to configure pseudo-distributed operation.\n",
        "\n",
        "**`etc/hadoop/core-site.xml`**\n",
        "```\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>fs.defaultFS</name>\n",
        "        <value>hdfs://localhost:9000</value>\n",
        "    </property>\n",
        "</configuration>\n",
        "```\n",
        "\n",
        "**`etc/hadoop/hdfs-site.xml`**\n",
        "```\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>dfs.replication</name>\n",
        "        <value>1</value>\n",
        "    </property>\n",
        "</configuration>\n",
        "```"
      ],
      "metadata": {
        "id": "KLmxLQeJSb4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "echo -e \"<configuration> \\n\\\n",
        "    <property> \\n\\\n",
        "        <name>fs.defaultFS</name> \\n\\\n",
        "        <value>hdfs://localhost:9000</value> \\n\\\n",
        "    </property> \\n\\\n",
        "</configuration>\" >hadoop-3.3.4/etc/hadoop/core-site.xml\n",
        "\n",
        "echo -e \"<configuration> \\n\\\n",
        "    <property> \\n\\\n",
        "        <name>dfs.replication</name> \\n\\\n",
        "        <value>1</value> \\n\\\n",
        "    </property> \\n\\\n",
        "</configuration>\" >hadoop-3.3.4/etc/hadoop/hdfs-site.xml"
      ],
      "metadata": {
        "id": "_n2d2lqXSLU1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check"
      ],
      "metadata": {
        "id": "5mdkNb-Cg9HW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat hadoop-3.3.4/etc/hadoop/hdfs-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ISxE4Gqg_LG",
        "outputId": "e2de5518-6901-4073-ebcf-7f2eccba5dec"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<configuration> \n",
            "    <property> \n",
            "        <name>dfs.replication</name> \n",
            "        <value>1</value> \n",
            "    </property> \n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set environment variables\n",
        "\n",
        "Add the following lines to the Hadoop configuration script `hadoop-env.sh`(the script is in `hadoop-3.3.4/sbin`).\n",
        "```\n",
        "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\n",
        "export HDFS_NAMENODE_USER=root\n",
        "export HDFS_DATANODE_USER=root\n",
        "export HDFS_SECONDARYNAMENODE_USER=root\n",
        "export YARN_RESOURCEMANAGER_USER=root\n",
        "export YARN_NODEMANAGER_USER=root\n",
        "```"
      ],
      "metadata": {
        "id": "kXbSKFyeMqr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "echo -e \"export HDFS_NAMENODE_USER=root \\n\\\n",
        "export HDFS_DATANODE_USER=root \\n\\\n",
        "export HDFS_SECONDARYNAMENODE_USER=root \\n\\\n",
        "export YARN_RESOURCEMANAGER_USER=root \\n\\\n",
        "export YARN_NODEMANAGER_USER=root\" >> hadoop-3.3.4/etc/hadoop/hadoop-env.sh"
      ],
      "metadata": {
        "id": "2_vn-TGyPe9V"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup localhost access via SSH key\n",
        "\n",
        "We are going to allow passphraseless sccess to `localhost` with a secure key.\n",
        "\n",
        "SSH must be installed and sshd must be running to use the Hadoop scripts that manage remote Hadoop daemons.\n"
      ],
      "metadata": {
        "id": "k2-Fdp73cF0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Install `openssh` and start server"
      ],
      "metadata": {
        "id": "-Uxmv3RdUwiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "apt-get install openssh-server\n",
        "echo 'StrictHostKeyChecking no' >> /etc/ssh/ssh_config\n",
        "/etc/init.d/ssh restart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOxz683FNuYH",
        "outputId": "3e90bad1-3b26-43fd-abd4-4f966433b3cd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "openssh-server is already the newest version (1:7.6p1-4ubuntu0.7).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 12 not upgraded.\n",
            " * Restarting OpenBSD Secure Shell server sshd\n",
            "   ...done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate key\n",
        "Generate SSH key that does not require a password."
      ],
      "metadata": {
        "id": "PYKoSlaENuyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "rm /root/.ssh/id_rsa\n",
        "ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n",
        "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
        "chmod 0600 ~/.ssh/authorized_keys"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHOjUaxHSsQD",
        "outputId": "a7fe221d-ba24-45b4-a2ad-f1856ff37562"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private rsa key pair.\n",
            "Your identification has been saved in /root/.ssh/id_rsa.\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub.\n",
            "The key fingerprint is:\n",
            "SHA256:dJfNcKhqqLWy7x7fxJpYHIsfvb1+WbXMj7LQ3b2fWXc root@93ad6cb45e12\n",
            "The key's randomart image is:\n",
            "+---[RSA 2048]----+\n",
            "|            ...  |\n",
            "|            .*   |\n",
            "|        . ..o o  |\n",
            "|       . ...    .|\n",
            "|       .S.    o o|\n",
            "|      ooo= . . =o|\n",
            "|     ooo= = . .+E|\n",
            "|    o .* * +. + O|\n",
            "|    .*= = o.== +o|\n",
            "+----[SHA256]-----+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check SSH connection to localhost\n",
        "\n",
        "The following command should output \"hi!\" if the connection works."
      ],
      "metadata": {
        "id": "FwA6rKpScnVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh localhost \"echo hi!\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqIRVxcfce0F",
        "outputId": "05c9daba-b392-44df-fa73-050be5ba50dc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launch a single-node Hadoop cluster"
      ],
      "metadata": {
        "id": "V68C4cDySyek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize the namenode"
      ],
      "metadata": {
        "id": "HTDPwnVlSbHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs namenode -format -nonInteractive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-aicnKKLVKQ",
        "outputId": "e4886fa9-d9c7-4f1c-d3c9-9fca8a5cb2d8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "namenode is running as process 7406.  Stop it first and ensure /tmp/hadoop-root-namenode.pid file is empty before retry.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo -e \"export JAVA_HOME=$(readlink -f $(which java) | sed 's/\\/bin\\/java$//') \\n\\\n",
        "export HDFS_NAMENODE_USER=root \\n\\\n",
        "export HDFS_DATANODE_USER=root \\n\\\n",
        "export HDFS_SECONDARYNAMENODE_USER=root \\n\\\n",
        "export YARN_RESOURCEMANAGER_USER=root \\n\\\n",
        "export YARN_NODEMANAGER_USER=root\" >> hadoop-3.3.4/etc/hadoop/hadoop-env.sh"
      ],
      "metadata": {
        "id": "8cb7lXMptVHb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start cluster"
      ],
      "metadata": {
        "id": "xMrEiLB_VAeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/sbin/start-dfs.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXHowFfFEwAF",
        "outputId": "073ff1bb-f1ca-4ae3-aa9b-fe8a3c974842"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting namenodes on [localhost]\n",
            "localhost: namenode is running as process 7406.  Stop it first and ensure /tmp/hadoop-root-namenode.pid file is empty before retry.\n",
            "Starting datanodes\n",
            "localhost: datanode is running as process 7540.  Stop it first and ensure /tmp/hadoop-root-datanode.pid file is empty before retry.\n",
            "Starting secondary namenodes [93ad6cb45e12]\n",
            "93ad6cb45e12: secondarynamenode is running as process 7759.  Stop it first and ensure /tmp/hadoop-root-secondarynamenode.pid file is empty before retry.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run some simple `hdfs` commands"
      ],
      "metadata": {
        "id": "CKRRbwDFv3ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# create directory \"my_dir\" in HDFS home \n",
        "hdfs dfs -mkdir /user\n",
        "hdfs dfs -mkdir /user/root # this is the \"home\" of user root on HDFS\n",
        "hdfs dfs -mkdir my_dir\n",
        "\n",
        "# upload file mnist_test.csv to my_dir\n",
        "hdfs dfs -put /content/sample_data/mnist_test.csv my_dir/\n",
        "\n",
        "# show contents of directory my_dir\n",
        "hdfs dfs -ls -h my_dir\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73wuvOJTxX4O",
        "outputId": "352c60de-78fb-4a4b-b631-5e54f492c6a0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "-rw-r--r--   1 root supergroup     17.4 M 2022-10-08 15:43 my_dir/mnist_test.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "mkdir: `/user': File exists\n",
            "mkdir: `/user/root': File exists\n",
            "mkdir: `my_dir': File exists\n",
            "put: `my_dir/mnist_test.csv': File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run some simple MrReduce jobs\n",
        "We're going to use the [streaming](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html) library. \n",
        "WIth this utility any executable can be used as the mapper and/or the reducer. "
      ],
      "metadata": {
        "id": "G3KBe4R65bl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Simplest MapReduce job\n",
        "\n",
        "We are going to use the Unix commands `cat` as mapper and `wc` as reducer so we don't need to code anything. The result will show a line with three values: the counts of lines, words, and characters in the input file(s).\n",
        "\n",
        "Input folder is `/user/my_user/my_dir/`, output folder `/user/my_user/output`.\n",
        "\n",
        "**Note**: the output folder should not exist because it is created by Hadoop (this is in acordance with Hadoop's principle of not overwriting data)."
      ],
      "metadata": {
        "id": "yVJA-3jSATGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "hdfs dfs -rm -r output >/dev/null 2>&1\n",
        "mapred streaming \\\n",
        "  -input my_dir \\\n",
        "  -output output \\\n",
        "  -mapper /bin/cat \\\n",
        "  -reducer /usr/bin/wc"
      ],
      "metadata": {
        "id": "VDuQYWGi5b7J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c18ea16-84ad-4b5c-e8c5-4a0a0760c3b9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-10-08 15:58:05,691 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2022-10-08 15:58:05,889 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2022-10-08 15:58:05,889 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2022-10-08 15:58:05,908 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-10-08 15:58:06,351 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2022-10-08 15:58:06,368 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2022-10-08 15:58:06,610 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1911765736_0001\n",
            "2022-10-08 15:58:06,611 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2022-10-08 15:58:06,809 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2022-10-08 15:58:06,811 INFO mapreduce.Job: Running job: job_local1911765736_0001\n",
            "2022-10-08 15:58:06,818 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2022-10-08 15:58:06,821 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2022-10-08 15:58:06,827 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-10-08 15:58:06,827 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-10-08 15:58:06,893 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2022-10-08 15:58:06,897 INFO mapred.LocalJobRunner: Starting task: attempt_local1911765736_0001_m_000000_0\n",
            "2022-10-08 15:58:06,945 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-10-08 15:58:06,947 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-10-08 15:58:06,984 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-10-08 15:58:06,997 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/root/my_dir/mnist_test.csv:0+18289443\n",
            "2022-10-08 15:58:07,043 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2022-10-08 15:58:07,115 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2022-10-08 15:58:07,115 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2022-10-08 15:58:07,115 INFO mapred.MapTask: soft limit at 83886080\n",
            "2022-10-08 15:58:07,115 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2022-10-08 15:58:07,115 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2022-10-08 15:58:07,119 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2022-10-08 15:58:07,121 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2022-10-08 15:58:07,127 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2022-10-08 15:58:07,130 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2022-10-08 15:58:07,131 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2022-10-08 15:58:07,131 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2022-10-08 15:58:07,132 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2022-10-08 15:58:07,132 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2022-10-08 15:58:07,134 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2022-10-08 15:58:07,135 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2022-10-08 15:58:07,135 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2022-10-08 15:58:07,135 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2022-10-08 15:58:07,136 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2022-10-08 15:58:07,137 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2022-10-08 15:58:07,308 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-10-08 15:58:07,309 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-10-08 15:58:07,326 INFO streaming.PipeMapRed: R/W/S=100/1/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-10-08 15:58:07,326 INFO streaming.PipeMapRed: Records R/W=90/1\n",
            "2022-10-08 15:58:07,394 INFO streaming.PipeMapRed: R/W/S=1000/803/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-10-08 15:58:07,650 INFO streaming.PipeMapRed: R/W/S=10000/9962/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-10-08 15:58:07,653 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-10-08 15:58:07,654 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-10-08 15:58:07,657 INFO mapred.LocalJobRunner: \n",
            "2022-10-08 15:58:07,658 INFO mapred.MapTask: Starting flush of map output\n",
            "2022-10-08 15:58:07,658 INFO mapred.MapTask: Spilling map output\n",
            "2022-10-08 15:58:07,658 INFO mapred.MapTask: bufstart = 0; bufend = 18319443; bufvoid = 104857600\n",
            "2022-10-08 15:58:07,658 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26174400(104697600); length = 39997/6553600\n",
            "2022-10-08 15:58:07,817 INFO mapreduce.Job: Job job_local1911765736_0001 running in uber mode : false\n",
            "2022-10-08 15:58:07,819 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2022-10-08 15:58:07,837 INFO mapred.MapTask: Finished spill 0\n",
            "2022-10-08 15:58:07,854 INFO mapred.Task: Task:attempt_local1911765736_0001_m_000000_0 is done. And is in the process of committing\n",
            "2022-10-08 15:58:07,860 INFO mapred.LocalJobRunner: Records R/W=90/1\n",
            "2022-10-08 15:58:07,860 INFO mapred.Task: Task 'attempt_local1911765736_0001_m_000000_0' done.\n",
            "2022-10-08 15:58:07,869 INFO mapred.Task: Final Counters for attempt_local1911765736_0001_m_000000_0: Counters: 23\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141408\n",
            "\t\tFILE: Number of bytes written=19142572\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=18289443\n",
            "\t\tHDFS: Number of bytes written=0\n",
            "\t\tHDFS: Number of read operations=5\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=1\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=10000\n",
            "\t\tMap output records=10000\n",
            "\t\tMap output bytes=18319443\n",
            "\t\tMap output materialized bytes=18359449\n",
            "\t\tInput split bytes=105\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=10000\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=31\n",
            "\t\tTotal committed heap usage (bytes)=457179136\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=18289443\n",
            "2022-10-08 15:58:07,869 INFO mapred.LocalJobRunner: Finishing task: attempt_local1911765736_0001_m_000000_0\n",
            "2022-10-08 15:58:07,870 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2022-10-08 15:58:07,874 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2022-10-08 15:58:07,875 INFO mapred.LocalJobRunner: Starting task: attempt_local1911765736_0001_r_000000_0\n",
            "2022-10-08 15:58:07,890 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-10-08 15:58:07,890 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-10-08 15:58:07,891 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-10-08 15:58:07,898 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3e947c19\n",
            "2022-10-08 15:58:07,899 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-10-08 15:58:07,928 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2022-10-08 15:58:07,931 INFO reduce.EventFetcher: attempt_local1911765736_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2022-10-08 15:58:08,006 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1911765736_0001_m_000000_0 decomp: 18359445 len: 18359449 to MEMORY\n",
            "2022-10-08 15:58:08,029 INFO reduce.InMemoryMapOutput: Read 18359445 bytes from map-output for attempt_local1911765736_0001_m_000000_0\n",
            "2022-10-08 15:58:08,031 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 18359445, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->18359445\n",
            "2022-10-08 15:58:08,033 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2022-10-08 15:58:08,034 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-10-08 15:58:08,034 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2022-10-08 15:58:08,041 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-10-08 15:58:08,041 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 18357610 bytes\n",
            "2022-10-08 15:58:08,122 INFO reduce.MergeManagerImpl: Merged 1 segments, 18359445 bytes to disk to satisfy reduce memory limit\n",
            "2022-10-08 15:58:08,123 INFO reduce.MergeManagerImpl: Merging 1 files, 18359449 bytes from disk\n",
            "2022-10-08 15:58:08,124 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2022-10-08 15:58:08,124 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-10-08 15:58:08,125 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 18357610 bytes\n",
            "2022-10-08 15:58:08,126 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-10-08 15:58:08,127 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/bin/wc]\n",
            "2022-10-08 15:58:08,132 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2022-10-08 15:58:08,135 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2022-10-08 15:58:08,193 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-10-08 15:58:08,194 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-10-08 15:58:08,199 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-10-08 15:58:08,251 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-10-08 15:58:08,661 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-10-08 15:58:08,667 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-10-08 15:58:08,667 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
            "2022-10-08 15:58:08,669 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-10-08 15:58:08,757 INFO mapred.Task: Task:attempt_local1911765736_0001_r_000000_0 is done. And is in the process of committing\n",
            "2022-10-08 15:58:08,762 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-10-08 15:58:08,762 INFO mapred.Task: Task attempt_local1911765736_0001_r_000000_0 is allowed to commit now\n",
            "2022-10-08 15:58:08,788 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1911765736_0001_r_000000_0' to hdfs://localhost:9000/user/root/output\n",
            "2022-10-08 15:58:08,791 INFO mapred.LocalJobRunner: Records R/W=10000/1 > reduce\n",
            "2022-10-08 15:58:08,791 INFO mapred.Task: Task 'attempt_local1911765736_0001_r_000000_0' done.\n",
            "2022-10-08 15:58:08,793 INFO mapred.Task: Final Counters for attempt_local1911765736_0001_r_000000_0: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=36860338\n",
            "\t\tFILE: Number of bytes written=37502021\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=18289443\n",
            "\t\tHDFS: Number of bytes written=26\n",
            "\t\tHDFS: Number of read operations=10\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=3\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=10000\n",
            "\t\tReduce shuffle bytes=18359449\n",
            "\t\tReduce input records=10000\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=10000\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=457179136\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=26\n",
            "2022-10-08 15:58:08,793 INFO mapred.LocalJobRunner: Finishing task: attempt_local1911765736_0001_r_000000_0\n",
            "2022-10-08 15:58:08,793 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2022-10-08 15:58:08,825 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2022-10-08 15:58:08,825 INFO mapreduce.Job: Job job_local1911765736_0001 completed successfully\n",
            "2022-10-08 15:58:08,859 INFO mapreduce.Job: Counters: 36\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=37001746\n",
            "\t\tFILE: Number of bytes written=56644593\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=36578886\n",
            "\t\tHDFS: Number of bytes written=26\n",
            "\t\tHDFS: Number of read operations=15\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=4\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=10000\n",
            "\t\tMap output records=10000\n",
            "\t\tMap output bytes=18319443\n",
            "\t\tMap output materialized bytes=18359449\n",
            "\t\tInput split bytes=105\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=10000\n",
            "\t\tReduce shuffle bytes=18359449\n",
            "\t\tReduce input records=10000\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=20000\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=31\n",
            "\t\tTotal committed heap usage (bytes)=914358272\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=18289443\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=26\n",
            "2022-10-08 15:58:08,863 INFO streaming.StreamJob: Output directory: output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the `output` directory contains the empty file `_SUCCESS`, this means that the job was successful. "
      ],
      "metadata": {
        "id": "UiZ6FH2gFfE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the output of the MapReduce job."
      ],
      "metadata": {
        "id": "kHPEoIIWFubx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -cat output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rB7FXYTbwNzm",
        "outputId": "a4568aaa-3165-4649-a040-63a4b688efed"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  10000   10000 18299443\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of words is in this case equal to the number of lines because there are no word separators (empty spaces) in the file, so each line is a word."
      ],
      "metadata": {
        "id": "BDObCPW2F39S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Another MapReduce example: filter a log file\n",
        "\n",
        "We're going to use a Linux logfile and look for the string `sshd` in a given position. \n",
        "\n",
        "The mapper `mapper.py` filters the file for the given string `sshd` at field 4. \n",
        "\n",
        "The job has no reducer (option `-reducer NONE`). Note that without a reducer the sorting and shuffling phase after the map phase is skipped.\n"
      ],
      "metadata": {
        "id": "BbosNo0TD3oH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the logfile `Linux_2k.log`:"
      ],
      "metadata": {
        "id": "iVdUuulwGzq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://raw.githubusercontent.com/logpai/loghub/master/Linux/Linux_2k.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJIm4SPZFPxy",
        "outputId": "4b25b28f-43fd-4cff-c2d5-e206d139eaa2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  7  209k    7 15524    0     0   170k      0  0:00:01 --:--:--  0:00:01  168k\r100  209k  100  209k    0     0  2228k      0 --:--:-- --:--:-- --:--:-- 2204k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -mkdir input\n",
        "hdfs dfs -put Linux_2k.log input/ \n",
        "hdfs dfs -rm -r output >/dev/null 2>&1"
      ],
      "metadata": {
        "id": "M1WgyQE3MYWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69ff854d-d66c-4f8a-b702-85ba4099066b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "mkdir: `input': File exists\n",
            "put: `input/Linux_2k.log': File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the mapper"
      ],
      "metadata": {
        "id": "ILUOCdzEH3Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "#!/usr/bin/env python\n",
        "import sys\n",
        "\n",
        "for line in sys.stdin:\n",
        "    # split the line into words\n",
        "    line = line.strip()\n",
        "    fields = line.split()\n",
        "    if (len(fields)>=5 and fields[4].startswith('sshd')):\n",
        "      print(line)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-rraIUdfdj0",
        "outputId": "20031f9c-3031-4506-ac9a-18800d02c63c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the script (after setting the correct permissions)"
      ],
      "metadata": {
        "id": "W8AxdFFPIuDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 700 mapper.py"
      ],
      "metadata": {
        "id": "QwOk_y7egbGM"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the first 10 lines"
      ],
      "metadata": {
        "id": "fhv95VzfIAnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -10 Linux_2k.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qf1dFqIKgoJ",
        "outputId": "d570f044-f698-478d-a54f-8f577414fed5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jun 14 15:16:01 combo sshd(pam_unix)[19939]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 \n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: check pass; user unknown\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 \n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20882]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20884]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20883]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20885]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20886]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20893]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the mapper in the shell (not using MapReduce):"
      ],
      "metadata": {
        "id": "eQ09Y1AqR6Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -100 Linux_2k.log| ./mapper.py "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ0kDRsigCC2",
        "outputId": "1cab36e8-60d9-460e-9ddc-a21fe0f49dd6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jun 14 15:16:01 combo sshd(pam_unix)[19939]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: check pass; user unknown\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20882]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20884]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20883]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20885]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20886]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20893]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20896]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20897]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20898]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23397]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23397]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23395]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23395]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23404]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23404]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23399]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23399]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23406]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23406]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23396]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23394]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23407]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23394]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23403]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23396]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23407]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23403]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23412]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23412]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:13:19 combo sshd(pam_unix)[23414]: check pass; user unknown\n",
            "Jun 15 12:13:19 combo sshd(pam_unix)[23414]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:13:20 combo sshd(pam_unix)[23416]: check pass; user unknown\n",
            "Jun 15 12:13:20 combo sshd(pam_unix)[23416]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23661]: check pass; user unknown\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23661]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23663]: check pass; user unknown\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23663]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23664]: check pass; user unknown\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23664]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:33 combo sshd(pam_unix)[23665]: check pass; user unknown\n",
            "Jun 15 14:53:33 combo sshd(pam_unix)[23665]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:34 combo sshd(pam_unix)[23669]: check pass; user unknown\n",
            "Jun 15 14:53:34 combo sshd(pam_unix)[23669]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23671]: check pass; user unknown\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23671]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23673]: check pass; user unknown\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23673]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23674]: check pass; user unknown\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23674]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:36 combo sshd(pam_unix)[23678]: check pass; user unknown\n",
            "Jun 15 14:53:36 combo sshd(pam_unix)[23678]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:36 combo sshd(pam_unix)[23677]: check pass; user unknown\n",
            "Jun 15 14:53:36 combo sshd(pam_unix)[23677]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24138]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24138]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24137]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24137]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24141]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24141]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24140]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24140]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24139]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24139]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 17 19:43:13 combo sshd(pam_unix)[30565]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.46.224.253  user=guest\n",
            "Jun 17 20:29:26 combo sshd(pam_unix)[30631]: session opened for user test by (uid=509)\n",
            "Jun 17 20:34:57 combo sshd(pam_unix)[30631]: session closed for user test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run the MapReduce job on the pseudo-cluster"
      ],
      "metadata": {
        "id": "DXbgh5g7OraF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "hdfs dfs -rm -r output >/dev/null 2>&1\n",
        "mapred streaming \\\n",
        "  -file mapper.py \\\n",
        "  -input input \\\n",
        "  -output output \\\n",
        "  -mapper mapper.py \\\n",
        "  -reducer NONE \n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7SEzMC2OqWW",
        "outputId": "2ae14df8-027a-41ff-c6a8-0c9fdbfb748b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "packageJobJar: [mapper.py] [] /tmp/streamjob11527518915332424217.jar tmpDir=null\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-10-08 15:58:25,431 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "2022-10-08 15:58:26,780 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2022-10-08 15:58:26,961 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2022-10-08 15:58:26,961 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2022-10-08 15:58:26,979 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-10-08 15:58:27,399 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2022-10-08 15:58:27,425 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2022-10-08 15:58:27,706 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1672820701_0001\n",
            "2022-10-08 15:58:27,706 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2022-10-08 15:58:28,020 INFO mapred.LocalDistributedCacheManager: Localized file:/content/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local1672820701_0001_bb1b6c90-27a5-4927-851d-71fec393bb74/mapper.py\n",
            "2022-10-08 15:58:28,160 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2022-10-08 15:58:28,161 INFO mapreduce.Job: Running job: job_local1672820701_0001\n",
            "2022-10-08 15:58:28,168 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2022-10-08 15:58:28,170 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2022-10-08 15:58:28,175 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-10-08 15:58:28,176 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-10-08 15:58:28,261 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2022-10-08 15:58:28,265 INFO mapred.LocalJobRunner: Starting task: attempt_local1672820701_0001_m_000000_0\n",
            "2022-10-08 15:58:28,299 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-10-08 15:58:28,302 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-10-08 15:58:28,329 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-10-08 15:58:28,340 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/root/input/Linux_2k.log:0+214486\n",
            "2022-10-08 15:58:28,389 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2022-10-08 15:58:28,458 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./mapper.py]\n",
            "2022-10-08 15:58:28,465 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2022-10-08 15:58:28,467 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2022-10-08 15:58:28,467 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2022-10-08 15:58:28,467 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2022-10-08 15:58:28,468 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2022-10-08 15:58:28,468 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2022-10-08 15:58:28,470 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2022-10-08 15:58:28,470 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2022-10-08 15:58:28,471 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2022-10-08 15:58:28,471 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2022-10-08 15:58:28,472 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2022-10-08 15:58:28,473 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2022-10-08 15:58:28,667 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-10-08 15:58:28,668 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-10-08 15:58:28,671 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-10-08 15:58:28,693 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-10-08 15:58:28,707 INFO streaming.PipeMapRed: Records R/W=1781/1\n",
            "2022-10-08 15:58:28,735 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-10-08 15:58:28,752 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-10-08 15:58:28,755 INFO mapred.LocalJobRunner: \n",
            "2022-10-08 15:58:28,827 INFO mapred.Task: Task:attempt_local1672820701_0001_m_000000_0 is done. And is in the process of committing\n",
            "2022-10-08 15:58:28,832 INFO mapred.LocalJobRunner: \n",
            "2022-10-08 15:58:28,832 INFO mapred.Task: Task attempt_local1672820701_0001_m_000000_0 is allowed to commit now\n",
            "2022-10-08 15:58:28,851 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1672820701_0001_m_000000_0' to hdfs://localhost:9000/user/root/output\n",
            "2022-10-08 15:58:28,853 INFO mapred.LocalJobRunner: Records R/W=1781/1\n",
            "2022-10-08 15:58:28,853 INFO mapred.Task: Task 'attempt_local1672820701_0001_m_000000_0' done.\n",
            "2022-10-08 15:58:28,860 INFO mapred.Task: Final Counters for attempt_local1672820701_0001_m_000000_0: Counters: 21\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=663\n",
            "\t\tFILE: Number of bytes written=642655\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=214486\n",
            "\t\tHDFS: Number of bytes written=85436\n",
            "\t\tHDFS: Number of read operations=9\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=3\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2000\n",
            "\t\tMap output records=677\n",
            "\t\tInput split bytes=102\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=19\n",
            "\t\tTotal committed heap usage (bytes)=425721856\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=214486\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=85436\n",
            "2022-10-08 15:58:28,860 INFO mapred.LocalJobRunner: Finishing task: attempt_local1672820701_0001_m_000000_0\n",
            "2022-10-08 15:58:28,860 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2022-10-08 15:58:29,166 INFO mapreduce.Job: Job job_local1672820701_0001 running in uber mode : false\n",
            "2022-10-08 15:58:29,167 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2022-10-08 15:58:29,170 INFO mapreduce.Job: Job job_local1672820701_0001 completed successfully\n",
            "2022-10-08 15:58:29,177 INFO mapreduce.Job: Counters: 21\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=663\n",
            "\t\tFILE: Number of bytes written=642655\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=214486\n",
            "\t\tHDFS: Number of bytes written=85436\n",
            "\t\tHDFS: Number of read operations=9\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=3\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2000\n",
            "\t\tMap output records=677\n",
            "\t\tInput split bytes=102\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=19\n",
            "\t\tTotal committed heap usage (bytes)=425721856\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=214486\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=85436\n",
            "2022-10-08 15:58:29,177 INFO streaming.StreamJob: Output directory: output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the result"
      ],
      "metadata": {
        "id": "iuZJ2ACzSTJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -ls output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhLA5HZEhfmT",
        "outputId": "352e033f-7a60-463e-f22f-5cd51678db0f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root supergroup          0 2022-10-08 15:58 output/_SUCCESS\n",
            "-rw-r--r--   1 root supergroup      85436 2022-10-08 15:58 output/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -cat output/part-00000 |head "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ffi4RvXnPH14",
        "outputId": "af6f63dc-13b0-4cc6-f443-3659bd3cd248"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jun 14 15:16:01 combo sshd(pam_unix)[19939]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\t\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: check pass; user unknown\t\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20882]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20884]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20883]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20885]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20886]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20893]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "cat: Unable to write to output stream.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aggregate data with MapReduce\n",
        "\n",
        "Following the example in [Hadoop Streaming/Aggregate package](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Aggregate_Package)"
      ],
      "metadata": {
        "id": "Sam22f-YT1xR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile myAggregatorForKeyCount.py\n",
        "#!/usr/local/bin/python\n",
        "import sys\n",
        "\n",
        "def generateLongCountToken(id):\n",
        "    return \"LongValueSum:\" + id + \"\\t\" + \"1\"\n",
        "\n",
        "def main(argv):\n",
        "    line = sys.stdin.readline()\n",
        "    try:\n",
        "        while line:\n",
        "            line = line[:-1]\n",
        "            fields = line.split()\n",
        "            s = fields[4].split('[')[0]\n",
        "            print(generateLongCountToken(s))\n",
        "            line = sys.stdin.readline()\n",
        "    except \"end of file\":\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "     main(sys.argv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMKEqUF1T-v9",
        "outputId": "a731278e-57b1-45db-a2f7-58cb3e503f52"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting myAggregatorForKeyCount.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set permissions"
      ],
      "metadata": {
        "id": "4b2S9K8FWDMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 700 myAggregatorForKeyCount.py"
      ],
      "metadata": {
        "id": "35DP8K2_WDYO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the mapper "
      ],
      "metadata": {
        "id": "r9M8lgxMVRYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -20 Linux_2k.log| ./myAggregatorForKeyCount.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-R7VNoTVRjL",
        "outputId": "362a3b7c-e90a-4fc6-c1ef-87d4d0c97c34"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:su(pam_unix)\t1\n",
            "LongValueSum:su(pam_unix)\t1\n",
            "LongValueSum:logrotate:\t1\n",
            "LongValueSum:su(pam_unix)\t1\n",
            "LongValueSum:su(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the MapReduce job"
      ],
      "metadata": {
        "id": "vOEpMFvsVRtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r output >/dev/null 2>&1\n",
        "mapred streaming \\\n",
        "  -file mapper.py \\\n",
        "  -file myAggregatorForKeyCount.py \\\n",
        "  -input input \\\n",
        "  -output output \\\n",
        "  -mapper myAggregatorForKeyCount.py \\\n",
        "  -reducer aggregate  \n",
        "  \n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwxHJ7yyVR34",
        "outputId": "686ba15f-60d4-4ff8-d377-96e1b9a8e6c3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "packageJobJar: [mapper.py, myAggregatorForKeyCount.py] [] /tmp/streamjob9018992628385811252.jar tmpDir=null\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-10-08 15:58:40,123 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "2022-10-08 15:58:41,487 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2022-10-08 15:58:41,611 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2022-10-08 15:58:41,611 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2022-10-08 15:58:41,631 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-10-08 15:58:41,988 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2022-10-08 15:58:42,011 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2022-10-08 15:58:42,305 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local368443522_0001\n",
            "2022-10-08 15:58:42,305 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2022-10-08 15:58:42,620 INFO mapred.LocalDistributedCacheManager: Localized file:/content/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local368443522_0001_012ba762-9622-444e-9aa5-815548114ea1/mapper.py\n",
            "2022-10-08 15:58:42,674 INFO mapred.LocalDistributedCacheManager: Localized file:/content/myAggregatorForKeyCount.py as file:/tmp/hadoop-root/mapred/local/job_local368443522_0001_9f08576a-0e89-44f1-b77f-b4525edc977d/myAggregatorForKeyCount.py\n",
            "2022-10-08 15:58:42,776 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2022-10-08 15:58:42,778 INFO mapreduce.Job: Running job: job_local368443522_0001\n",
            "2022-10-08 15:58:42,790 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2022-10-08 15:58:42,793 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2022-10-08 15:58:42,798 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-10-08 15:58:42,798 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-10-08 15:58:42,867 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2022-10-08 15:58:42,871 INFO mapred.LocalJobRunner: Starting task: attempt_local368443522_0001_m_000000_0\n",
            "2022-10-08 15:58:42,907 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-10-08 15:58:42,907 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-10-08 15:58:42,945 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-10-08 15:58:42,976 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/root/input/Linux_2k.log:0+214486\n",
            "2022-10-08 15:58:43,014 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2022-10-08 15:58:43,080 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2022-10-08 15:58:43,081 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2022-10-08 15:58:43,081 INFO mapred.MapTask: soft limit at 83886080\n",
            "2022-10-08 15:58:43,081 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2022-10-08 15:58:43,081 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2022-10-08 15:58:43,085 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2022-10-08 15:58:43,101 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./myAggregatorForKeyCount.py]\n",
            "2022-10-08 15:58:43,112 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2022-10-08 15:58:43,115 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2022-10-08 15:58:43,115 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2022-10-08 15:58:43,116 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2022-10-08 15:58:43,117 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2022-10-08 15:58:43,117 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2022-10-08 15:58:43,119 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2022-10-08 15:58:43,119 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2022-10-08 15:58:43,119 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2022-10-08 15:58:43,120 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2022-10-08 15:58:43,120 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2022-10-08 15:58:43,121 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2022-10-08 15:58:43,270 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-10-08 15:58:43,270 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-10-08 15:58:43,284 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-10-08 15:58:43,306 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-10-08 15:58:43,310 INFO streaming.PipeMapRed: Records R/W=1201/1\n",
            "2022-10-08 15:58:43,334 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-10-08 15:58:43,351 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-10-08 15:58:43,354 INFO mapred.LocalJobRunner: \n",
            "2022-10-08 15:58:43,354 INFO mapred.MapTask: Starting flush of map output\n",
            "2022-10-08 15:58:43,354 INFO mapred.MapTask: Spilling map output\n",
            "2022-10-08 15:58:43,354 INFO mapred.MapTask: bufstart = 0; bufend = 48923; bufvoid = 104857600\n",
            "2022-10-08 15:58:43,355 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206400(104825600); length = 7997/6553600\n",
            "2022-10-08 15:58:43,445 INFO mapred.MapTask: Finished spill 0\n",
            "2022-10-08 15:58:43,467 INFO mapred.Task: Task:attempt_local368443522_0001_m_000000_0 is done. And is in the process of committing\n",
            "2022-10-08 15:58:43,473 INFO mapred.LocalJobRunner: Records R/W=1201/1\n",
            "2022-10-08 15:58:43,473 INFO mapred.Task: Task 'attempt_local368443522_0001_m_000000_0' done.\n",
            "2022-10-08 15:58:43,479 INFO mapred.Task: Final Counters for attempt_local368443522_0001_m_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1533\n",
            "\t\tFILE: Number of bytes written=641733\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=214486\n",
            "\t\tHDFS: Number of bytes written=0\n",
            "\t\tHDFS: Number of read operations=5\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=1\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2000\n",
            "\t\tMap output records=2000\n",
            "\t\tMap output bytes=48923\n",
            "\t\tMap output materialized bytes=782\n",
            "\t\tInput split bytes=102\n",
            "\t\tCombine input records=2000\n",
            "\t\tCombine output records=30\n",
            "\t\tSpilled Records=30\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=18\n",
            "\t\tTotal committed heap usage (bytes)=475004928\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=214486\n",
            "2022-10-08 15:58:43,479 INFO mapred.LocalJobRunner: Finishing task: attempt_local368443522_0001_m_000000_0\n",
            "2022-10-08 15:58:43,479 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2022-10-08 15:58:43,482 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2022-10-08 15:58:43,482 INFO mapred.LocalJobRunner: Starting task: attempt_local368443522_0001_r_000000_0\n",
            "2022-10-08 15:58:43,494 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-10-08 15:58:43,496 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-10-08 15:58:43,496 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-10-08 15:58:43,500 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@58593c7f\n",
            "2022-10-08 15:58:43,501 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-10-08 15:58:43,526 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2022-10-08 15:58:43,546 INFO reduce.EventFetcher: attempt_local368443522_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2022-10-08 15:58:43,587 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local368443522_0001_m_000000_0 decomp: 778 len: 782 to MEMORY\n",
            "2022-10-08 15:58:43,591 INFO reduce.InMemoryMapOutput: Read 778 bytes from map-output for attempt_local368443522_0001_m_000000_0\n",
            "2022-10-08 15:58:43,593 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 778, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->778\n",
            "2022-10-08 15:58:43,598 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2022-10-08 15:58:43,599 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-10-08 15:58:43,599 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2022-10-08 15:58:43,608 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-10-08 15:58:43,608 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 760 bytes\n",
            "2022-10-08 15:58:43,610 INFO reduce.MergeManagerImpl: Merged 1 segments, 778 bytes to disk to satisfy reduce memory limit\n",
            "2022-10-08 15:58:43,610 INFO reduce.MergeManagerImpl: Merging 1 files, 782 bytes from disk\n",
            "2022-10-08 15:58:43,611 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2022-10-08 15:58:43,611 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-10-08 15:58:43,612 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 760 bytes\n",
            "2022-10-08 15:58:43,613 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-10-08 15:58:43,737 INFO mapred.Task: Task:attempt_local368443522_0001_r_000000_0 is done. And is in the process of committing\n",
            "2022-10-08 15:58:43,741 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-10-08 15:58:43,741 INFO mapred.Task: Task attempt_local368443522_0001_r_000000_0 is allowed to commit now\n",
            "2022-10-08 15:58:43,762 INFO output.FileOutputCommitter: Saved output of task 'attempt_local368443522_0001_r_000000_0' to hdfs://localhost:9000/user/root/output\n",
            "2022-10-08 15:58:43,764 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2022-10-08 15:58:43,764 INFO mapred.Task: Task 'attempt_local368443522_0001_r_000000_0' done.\n",
            "2022-10-08 15:58:43,765 INFO mapred.Task: Final Counters for attempt_local368443522_0001_r_000000_0: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=3129\n",
            "\t\tFILE: Number of bytes written=642515\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=214486\n",
            "\t\tHDFS: Number of bytes written=326\n",
            "\t\tHDFS: Number of read operations=10\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=3\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=30\n",
            "\t\tReduce shuffle bytes=782\n",
            "\t\tReduce input records=30\n",
            "\t\tReduce output records=30\n",
            "\t\tSpilled Records=30\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=475004928\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=326\n",
            "2022-10-08 15:58:43,765 INFO mapred.LocalJobRunner: Finishing task: attempt_local368443522_0001_r_000000_0\n",
            "2022-10-08 15:58:43,766 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2022-10-08 15:58:43,784 INFO mapreduce.Job: Job job_local368443522_0001 running in uber mode : false\n",
            "2022-10-08 15:58:43,785 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2022-10-08 15:58:44,786 INFO mapreduce.Job: Job job_local368443522_0001 completed successfully\n",
            "2022-10-08 15:58:44,797 INFO mapreduce.Job: Counters: 36\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=4662\n",
            "\t\tFILE: Number of bytes written=1284248\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=428972\n",
            "\t\tHDFS: Number of bytes written=326\n",
            "\t\tHDFS: Number of read operations=15\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=4\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2000\n",
            "\t\tMap output records=2000\n",
            "\t\tMap output bytes=48923\n",
            "\t\tMap output materialized bytes=782\n",
            "\t\tInput split bytes=102\n",
            "\t\tCombine input records=2000\n",
            "\t\tCombine output records=30\n",
            "\t\tReduce input groups=30\n",
            "\t\tReduce shuffle bytes=782\n",
            "\t\tReduce input records=30\n",
            "\t\tReduce output records=30\n",
            "\t\tSpilled Records=60\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=18\n",
            "\t\tTotal committed heap usage (bytes)=950009856\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=214486\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=326\n",
            "2022-10-08 15:58:44,797 INFO streaming.StreamJob: Output directory: output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check result"
      ],
      "metadata": {
        "id": "NkuYUkh5W0Je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -ls output\n",
        "hdfs dfs -cat output/part-00000 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET3KCfX1UC2u",
        "outputId": "27bb5de8-c511-41be-a78e-3b6a27a967c6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root supergroup          0 2022-10-08 15:58 output/_SUCCESS\n",
            "-rw-r--r--   1 root supergroup        326 2022-10-08 15:58 output/part-00000\n",
            "--\t1\n",
            "bluetooth:\t2\n",
            "cups:\t12\n",
            "ftpd\t916\n",
            "gdm(pam_unix)\t2\n",
            "gdm-binary\t1\n",
            "gpm\t2\n",
            "hcid\t1\n",
            "irqbalance:\t1\n",
            "kernel:\t76\n",
            "klogind\t46\n",
            "login(pam_unix)\t2\n",
            "logrotate:\t43\n",
            "named\t16\n",
            "network:\t2\n",
            "nfslock:\t1\n",
            "portmap:\t1\n",
            "random:\t1\n",
            "rc:\t1\n",
            "rpc.statd\t1\n",
            "rpcidmapd:\t1\n",
            "sdpd\t1\n",
            "snmpd\t1\n",
            "sshd(pam_unix)\t677\n",
            "su(pam_unix)\t172\n",
            "sysctl:\t1\n",
            "syslog:\t2\n",
            "syslogd\t7\n",
            "udev\t8\n",
            "xinetd\t2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretty-print table of aggregated data"
      ],
      "metadata": {
        "id": "Vj9qz8wSa1w0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -get output/part-00000 result # download results file\n",
        "column -t result|sort -k2nr # sort by field 2 numerically in descending order"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8IYl4hAZhZm",
        "outputId": "24b3cb3d-9cc0-4b1f-ea91-a497d487453f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ftpd             916\n",
            "sshd(pam_unix)   677\n",
            "su(pam_unix)     172\n",
            "kernel:          76\n",
            "klogind          46\n",
            "logrotate:       43\n",
            "named            16\n",
            "cups:            12\n",
            "udev             8\n",
            "syslogd          7\n",
            "bluetooth:       2\n",
            "gdm(pam_unix)    2\n",
            "gpm              2\n",
            "login(pam_unix)  2\n",
            "network:         2\n",
            "syslog:          2\n",
            "xinetd           2\n",
            "--               1\n",
            "gdm-binary       1\n",
            "hcid             1\n",
            "irqbalance:      1\n",
            "nfslock:         1\n",
            "portmap:         1\n",
            "random:          1\n",
            "rc:              1\n",
            "rpcidmapd:       1\n",
            "rpc.statd        1\n",
            "sdpd             1\n",
            "snmpd            1\n",
            "sysctl:          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "get: `result': File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop cluster\n",
        "\n",
        "When you're done with your computations, you can shut down the Hadoop cluster."
      ],
      "metadata": {
        "id": "IF6-Z5RotAcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./hadoop-3.3.4/sbin/stop-dfs.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoIYG5NlsIMv",
        "outputId": "db877767-8c99-4786-cddf-d3eb6111e393"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping namenodes on [localhost]\n",
            "Stopping datanodes\n",
            "Stopping secondary namenodes [93ad6cb45e12]\n"
          ]
        }
      ]
    }
  ]
}